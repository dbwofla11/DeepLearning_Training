{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8X4CP/eJvJxAVWg9PdLaL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbwofla11/DeepLearning_Training/blob/master/ANN/FFNN%EA%B5%AC%ED%98%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1일차 ✈  단일 입력 FFNN 구현하기\n",
        "* TensorFlow없이 구현하기\n",
        "* TensorFlow있이 구현하기"
      ],
      "metadata": {
        "id": "agq30CiwsMOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 단일 은닉층 FFNN ( AND 문제 )\n"
      ],
      "metadata": {
        "id": "Si7UPhi72wMr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S12Gk4QcpDSS",
        "outputId": "c92b21d7-3051-4c00-8b97-de545be0b638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.2524195337104239\n",
            "Epoch 100, Loss: 0.25104141770850463\n",
            "Epoch 200, Loss: 0.24990062091141851\n",
            "Epoch 300, Loss: 0.2488181372960737\n",
            "Epoch 400, Loss: 0.2476699969208191\n",
            "Epoch 500, Loss: 0.24633939817688333\n",
            "Epoch 600, Loss: 0.24470439389189913\n",
            "Epoch 700, Loss: 0.2426358738623121\n",
            "Epoch 800, Loss: 0.24000234616667976\n",
            "Epoch 900, Loss: 0.23667591137143057\n",
            "Epoch 1000, Loss: 0.23253531622210533\n",
            "Epoch 1100, Loss: 0.2274703119299119\n",
            "Epoch 1200, Loss: 0.22139943971359757\n",
            "Epoch 1300, Loss: 0.21430965539100438\n",
            "Epoch 1400, Loss: 0.20630754155913214\n",
            "Epoch 1500, Loss: 0.19764760816717702\n",
            "Epoch 1600, Loss: 0.18869810313455887\n",
            "Epoch 1700, Loss: 0.17984068834560382\n",
            "Epoch 1800, Loss: 0.17135241105248455\n",
            "Epoch 1900, Loss: 0.16332833072415504\n",
            "Epoch 2000, Loss: 0.15566323481905953\n",
            "Epoch 2100, Loss: 0.14807315330677967\n",
            "Epoch 2200, Loss: 0.14013636418981104\n",
            "Epoch 2300, Loss: 0.13135815302100529\n",
            "Epoch 2400, Loss: 0.121283166301457\n",
            "Epoch 2500, Loss: 0.10966374782395863\n",
            "Epoch 2600, Loss: 0.0966411344206125\n",
            "Epoch 2700, Loss: 0.08284520132096568\n",
            "Epoch 2800, Loss: 0.06928293528625246\n",
            "Epoch 2900, Loss: 0.05695358061201196\n",
            "Epoch 3000, Loss: 0.046439963546118214\n",
            "Epoch 3100, Loss: 0.03783188579474757\n",
            "Epoch 3200, Loss: 0.030924195602543433\n",
            "Epoch 3300, Loss: 0.025419838742571574\n",
            "Epoch 3400, Loss: 0.02103425886693555\n",
            "Epoch 3500, Loss: 0.017528753502059406\n",
            "Epoch 3600, Loss: 0.014713096400831672\n",
            "Epoch 3700, Loss: 0.012438676487074446\n",
            "Epoch 3800, Loss: 0.010590193723742942\n",
            "Epoch 3900, Loss: 0.009078313231878498\n",
            "Epoch 4000, Loss: 0.007833732386254823\n",
            "Epoch 4100, Loss: 0.006802547200136689\n",
            "Epoch 4200, Loss: 0.005942682942883234\n",
            "Epoch 4300, Loss: 0.005221161263617177\n",
            "Epoch 4400, Loss: 0.004612013349255062\n",
            "Epoch 4500, Loss: 0.004094687621456608\n",
            "Epoch 4600, Loss: 0.003652834123804125\n",
            "Epoch 4700, Loss: 0.003273375036484495\n",
            "Epoch 4800, Loss: 0.002945792263793375\n",
            "Epoch 4900, Loss: 0.0026615797125433723\n",
            "Epoch 5000, Loss: 0.0024138206567759857\n",
            "Epoch 5100, Loss: 0.0021968602958395067\n",
            "Epoch 5200, Loss: 0.0020060509517367906\n",
            "Epoch 5300, Loss: 0.0018375528764232939\n",
            "Epoch 5400, Loss: 0.0016881777901113413\n",
            "Epoch 5500, Loss: 0.0015552653873303049\n",
            "Epoch 5600, Loss: 0.0014365853874986063\n",
            "Epoch 5700, Loss: 0.0013302594666285091\n",
            "Epoch 5800, Loss: 0.0012346987331567894\n",
            "Epoch 5900, Loss: 0.0011485534132692089\n",
            "Epoch 6000, Loss: 0.0010706721710145445\n",
            "Epoch 6100, Loss: 0.0010000690666556926\n",
            "Epoch 6200, Loss: 0.0009358965982279006\n",
            "Epoch 6300, Loss: 0.0008774236097952065\n",
            "Epoch 6400, Loss: 0.0008240171105091455\n",
            "Epoch 6500, Loss: 0.0007751272500485121\n",
            "Epoch 6600, Loss: 0.000730274852433106\n",
            "Epoch 6700, Loss: 0.0006890410321511534\n",
            "Epoch 6800, Loss: 0.000651058512016604\n",
            "Epoch 6900, Loss: 0.0006160043372383283\n",
            "Epoch 7000, Loss: 0.0005835937394467306\n",
            "Epoch 7100, Loss: 0.0005535749514026644\n",
            "Epoch 7200, Loss: 0.0005257248105048249\n",
            "Epoch 7300, Loss: 0.0004998450190897856\n",
            "Epoch 7400, Loss: 0.00047575895348535497\n",
            "Epoch 7500, Loss: 0.00045330893307574474\n",
            "Epoch 7600, Loss: 0.0004323538762331722\n",
            "Epoch 7700, Loss: 0.0004127672826204728\n",
            "Epoch 7800, Loss: 0.0003944354916662156\n",
            "Epoch 7900, Loss: 0.000377256175423872\n",
            "Epoch 8000, Loss: 0.00036113703091898285\n",
            "Epoch 8100, Loss: 0.00034599464275524743\n",
            "Epoch 8200, Loss: 0.0003317534914246381\n",
            "Epoch 8300, Loss: 0.00031834508663418376\n",
            "Epoch 8400, Loss: 0.00030570720817163353\n",
            "Epoch 8500, Loss: 0.00029378323950381396\n",
            "Epoch 8600, Loss: 0.0002825215815314746\n",
            "Epoch 8700, Loss: 0.0002718751357911671\n",
            "Epoch 8800, Loss: 0.00026180084796152675\n",
            "Epoch 8900, Loss: 0.00025225930384995747\n",
            "Epoch 9000, Loss: 0.00024321437114837845\n",
            "Epoch 9100, Loss: 0.0002346328811877376\n",
            "Epoch 9200, Loss: 0.00022648434571911587\n",
            "Epoch 9300, Loss: 0.00021874070442757227\n",
            "Epoch 9400, Loss: 0.0002113760994627853\n",
            "Epoch 9500, Loss: 0.0002043666737639614\n",
            "Epoch 9600, Loss: 0.0001976903903789428\n",
            "Epoch 9700, Loss: 0.0001913268703395023\n",
            "Epoch 9800, Loss: 0.0001852572469661399\n",
            "Epoch 9900, Loss: 0.0001794640347438343\n",
            "\n",
            "Predictions:\n",
            "[[0.00417187]\n",
            " [0.98941766]\n",
            " [0.98704817]\n",
            " [0.01996456]]\n"
          ]
        }
      ],
      "source": [
        "# 단일 은닉층 모델 AND 문제\n",
        "import numpy as np\n",
        "\n",
        "def sigmoide(x : float):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def derivada_sigmoide(x : float):\n",
        "  return x * (1 - x)\n",
        "\n",
        "class FFNN:\n",
        "  def __init__(self , input_Size , hidden_Size , output_Size):\n",
        "    self.input_Size = input_Size\n",
        "    self.hidden_Size = hidden_Size\n",
        "    self.output_Size = output_Size\n",
        "\n",
        "    # 가중치 초기화\n",
        "    self.W1 = np.random.randn(self.input_Size , self.hidden_Size) # I -> H\n",
        "    self.W2 = np.random.randn(self.hidden_Size , self.output_Size) # H -> O\n",
        "\n",
        "    self.b1 = np.zeros((1 , self.hidden_Size)) # H bias\n",
        "    self.b2 = np.zeros((1 , self.output_Size)) # O bias\n",
        "\n",
        "# 포워드와 벡워드만 잘 수식보면서 이해하면 됨 ( 수식대로만 친거임 )\n",
        "  def forward(self , X):\n",
        "      self.z1 = np.dot(X , self.W1) + self.b1\n",
        "      self.a1 = sigmoide(self.z1)\n",
        "\n",
        "      self.z2 = np.dot(self.a1 , self.W2) + self.b2\n",
        "      self.a2 = sigmoide(self.z2)\n",
        "\n",
        "      return self.a2\n",
        "\n",
        "  def backward(self , X , Y , learning_rate):\n",
        "      m = X.shape[0] # 샘플 수\n",
        "\n",
        "      # 출력층 오차\n",
        "      dZ2 = self.a2 - Y # 예측 - 실제\n",
        "      dW2 = (1 / m) * np.dot(self.a1.T , dZ2)\n",
        "      db2 = (1 / m) * np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "      # 은닉층 오차\n",
        "      dA1 = np.dot(dZ2 , self.W2.T)\n",
        "      dZ1 = dA1 * derivada_sigmoide(self.a1)\n",
        "      dW1 = (1 / m) * np.dot(X.T , dZ1)\n",
        "      db1 = (1 / m) * np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "      # 가중치와 편향 업데이트\n",
        "      self.W1 -= learning_rate * dW1\n",
        "      self.b1 -= learning_rate * db1\n",
        "      self.W2 -= learning_rate * dW2\n",
        "      self.b2 -= learning_rate * db2\n",
        "\n",
        "\n",
        "  def train(self, X , Y , epochs , learning_rate):\n",
        "      for epoch in range(epochs):\n",
        "        self.forward(X)\n",
        "        self.backward(X , Y , learning_rate)\n",
        "\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "          loss = np.mean(np.square(Y - self.a2))  # MSE (Mean Squared Error) 실제와 예측\n",
        "          print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "\n",
        "  def predict(self , X):\n",
        "      return self.forward(X)\n",
        "\n",
        "# 예시 데이터\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # 입력 데이터 (AND 문제)\n",
        "Y = np.array([[0], [1], [1], [0]])  # 출력 데이터 (AND 문제에 대한 결과)\n",
        "\n",
        "# 네트워크 인스턴스 생성 (2개 입력, 4개 은닉층 뉴런, 1개 출력)\n",
        "input_size = 2\n",
        "hidden_size = 4\n",
        "output_size = 1\n",
        "nn = FFNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# 모델 학습\n",
        "nn.train(X, Y, epochs=10000, learning_rate=0.1)\n",
        "\n",
        "# 예측\n",
        "predictions = nn.predict(X)\n",
        "print(\"\\nPredictions:\")\n",
        "print(predictions)"
      ]
    }
  ]
}